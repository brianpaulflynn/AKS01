# Define AKS Cluster
resource "azurerm_kubernetes_cluster" "aks_cluster" {
  name                = "aks-cluster"
  location            = "<location>"
  resource_group_name = azurerm_resource_group.aks_rg.name
  dns_prefix          = "aks-cluster"

  identity {
    type = "UserAssigned"
    identity_ids = [
      azurerm_user_assigned_identity.aks_cluster_identity.id
    ]
  }

  default_node_pool {
    name            = "default"
    node_count      = 3
    availability_zones = [1, 2, 3]
    enable_auto_scaling = true
    min_count       = 3
    max_count       = 100
    max_pods        = 32
    mode            = "System"
    os_disk_size_gb = 30
    os_type         = "Linux"
    enable_taint     = true
    taints = [
      {
        key    = "CriticalAddonsOnly"
        value  = "true"
        effect = "NoSchedule"
      }
    ]
  }

  agent_pool_profile {
    name            = "user-pool-1"
    node_count      = 3
    availability_zones = [1, 2, 3]
    enable_auto_scaling = true
    min_count       = 3
    max_count       = 600
    max_pods        = 32
    mode            = "User"
    os_disk_size_gb = 30
    os_type         = "Linux"
    enable_taint     = true
    taints = [
      {
        key    = "pool1"
        value  = "true"
        effect = "NoSchedule"
      }
    ]
  }

  agent_pool_profile {
    name            = "user-pool-2"
    node_count      = 3
    availability_zones = [1, 2, 3]
    enable_auto_scaling = true
    min_count       = 3
    max_count       = 300
    max_pods        = 32
    mode            = "User"
    os_disk_size_gb = 30
    os_type         = "Linux"
    tolerations = [
      {
        key      = "pool1"
        operator = "Exists"
        effect   = "NoExecute"
      }
    ]
  }

  network_profile {
    network_plugin = "azure"
  }

  addon_profile {
    azure_policy {
      enabled                     = true
      user_assigned_identity_id   = azurerm_user_assigned_identity.aks_cluster_identity.id
      log_analytics_workspace_id  = azurerm_log_analytics_workspace.log_analytics_workspace.id
      send_logs_to_workspace      = true
      send_metrics_to_workspace   = true
    }

    azure_monitor {
      enabled                    = true
      user_assigned_identity_id  = azurerm_user_assigned_identity.aks_cluster_identity.id
      log_analytics_workspace_id = azurerm_log_analytics_workspace.log_analytics_workspace.id
    }

    oms_agent {
      enabled                   = true
      log_analytics_workspace_id = azurerm_log_analytics_workspace.log_analytics_workspace.id
      send_logs_to_workspace    = true
      send_metrics_to_workspace = true
    }

    container_insights {
      enabled                   = true
      log_analytics_workspace_id = azurerm_log_analytics_workspace.log_analytics_workspace.id
      send_logs_to_workspace    = true
      send_metrics_to_workspace = true
    }

    azure_firewall {
      enabled                   = true
      user_assigned_identity_id = azurerm_user_assigned_identity.aks_cluster_identity.id
      log_analytics_workspace_id = azurerm_log_analytics_workspace.log_analytics_workspace.id
      send_logs_to_workspace    = true
      send_metrics_to_workspace = true
    }

    defender {
      enabled                   = true
      user_assigned_identity_id = azurerm_user_assigned_identity.aks_cluster_identity.id
      log_analytics_workspace_id = azurerm_log_analytics_workspace.log_analytics_workspace.id
      send_logs_to_workspace    = true
      send_metrics_to_workspace = true
    }

    acr_integration {
      enabled                   = true
      user_assigned_identity_id = azurerm_user_assigned_identity.aks_cluster_identity.id
      log_analytics_workspace_id = azurerm_log_analytics_workspace.log_analytics_workspace.id
      send_logs_to_workspace    = true
      send_metrics_to_workspace = true
    }
  }

  private_cluster_enabled = true

  aad_profile {
    managed = true
    enable_azure_rbac = true
  }

  enable_defender_profile = true
  enable_local_auth      = false
  enable_pod_security_policy = true

  enable_keyvault = true

  disable_public_fqdn = false
  outbound_type       = "loadBalancer"

  role_based_access_control {
    enabled = true
  }

  upgrade_policy {
    mode = "Manual"
  }

  pod_security_policy {
    enabled  = true
    name     = "restricted"
    metadata = <<EOF
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: restricted
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
    - ALL
  allowedCapabilities: []
  volumes:
    - 'configMap'
    - 'downwardAPI'
    - 'emptyDir'
    - 'persistentVolumeClaim'
    - 'secret'
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: MustRunAsNonRoot
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: MustRunAs
    ranges:
      - min: 1
        max: 65535
  fsGroup:
    rule: MustRunAs
    ranges:
      - min: 1
        max: 65535
EOF
  }

  auto_scaler_profile {
    name = "default"
    scan_interval = "5m"
    scale_down_delay_after_add = "10m"
    scale_down_delay_after_delete = "10m"
    scale_down_delay_after_failure = "3m"
    scale_down_unneeded_time = "20m"
    scale_down_unready_time = "10m"
    scale_down_evaluation_interval = "1m"

    auto_scaler_trigger {
      metric_name        = "cpu"
      metric_type        = "Resource"
      operator           = "GreaterThanOrEqualTo"
      threshold          = 50
      direction          = "Increase"
      cooldown           = "5m"
      max_replicas       = 10
      min_replicas       = 1
    }
  }

  service_principal {
    client_id     = "<service_principal_client_id>"
    client_secret = "<service_principal_client_secret>"
  }

}
# Define AKS Cluster Node Pools

resource "azurerm_kubernetes_cluster_node_pool" "default_node_pool" {
  name                  = "default"
  kubernetes_cluster_id = azurerm_kubernetes_cluster.aks_cluster.id
  vm_size               = "Standard_D2_v2"
  availability_zones    = [1, 2, 3]
  enable_auto_scaling   = true
  min_count             = 3
  max_count             = 100
  max_pods              = 32
  mode                  = "System"
  os_disk_size_gb       = 30
  os_type               = "Linux"
}

resource "azurerm_kubernetes_cluster_node_pool" "user_node_pool_1" {
  name                  = "user-pool-1"
  kubernetes_cluster_id = azurerm_kubernetes_cluster.aks_cluster.id
  vm_size               = "Standard_D2_v2"
  availability_zones    = [1, 2, 3]
  enable_auto_scaling   = true
  min_count             = 3
  max_count             = 600
  max_pods              = 32
  mode                  = "System"
  os_disk_size_gb       = 30
  os_type               = "Linux"
  node_taints           = ["pool=user-pool-2:NoExecute"]
}

resource "azurerm_kubernetes_cluster_node_pool" "user_node_pool_2" {
  name                  = "user-pool-2"
  kubernetes_cluster_id = azurerm_kubernetes_cluster.aks_cluster.id
  vm_size               = "Standard_D2_v2"
  availability_zones    = [1, 2, 3]
  enable_auto_scaling   = true
  min_count             = 3
  max_count             = 300
  max_pods              = 32
  mode                  = "System"
  os_disk_size_gb       = 30
  os_type               = "Linux"
  node_taints           = ["pool=user-pool-1:NoSchedule"]
}

# Define Auto-Scalers

resource "azurerm_kubernetes_cluster_node_pool_aad_auto_scaler" "default_node_pool_auto_scaler" {
  kubernetes_cluster_id  = azurerm_kubernetes_cluster.aks_cluster.id
  node_pool_name         = azurerm_kubernetes_cluster_node_pool.default_node_pool.name
  maximum_replica_count  = 100
  minimum_replica_count  = 1
  target_utilization     = 50
}

resource "azurerm_kubernetes_cluster_node_pool_aad_auto_scaler" "user_node_pool_1_auto_scaler" {
  kubernetes_cluster_id  = azurerm_kubernetes_cluster.aks_cluster.id
  node_pool_name         = azurerm_kubernetes_cluster_node_pool.user_node_pool_1.name
  maximum_replica_count  = 600
  minimum_replica_count  = 3
  target_utilization     = 50
}

resource "azurerm_kubernetes_cluster_node_pool_aad_auto_scaler" "user_node_pool_2_auto_scaler" {
  kubernetes_cluster_id  = azurerm_kubernetes_cluster.aks_cluster.id
  node_pool_name         = azurerm_kubernetes_cluster_node_pool.user_node_pool_2.name
  maximum_replica_count  = 300
  minimum_replica_count  = 3
  target_utilization     = 50
}

# Integrate AKS Cluster with Firewall
resource "azurerm_subnet_network_security_group_association" "aks_firewall_association" {
  subnet_id                 = azurerm_subnet.aks_subnet.id
  network_security_group_id = azurerm_network_security_group.firewall_nsg.id
}

resource "azurerm_kubernetes_cluster_extension" "aks_extension" {
  name                = "aks-extension"
  cluster_name        = azurerm_kubernetes_cluster.aks_cluster.name
  resource_group_name = azurerm_resource_group.aks_rg.name

  extension_type = "Microsoft.ContainerService/AKS"

  extension_name                    = "FirewallIntegration"
  extension_version                 = "1.0"
  extension_published_date          = "2022-01-01"
  extension_automatic_upgrade_minor_version_enabled = false
}

# Integrate AKS Cluster with Web Application Gateway
resource "azurerm_subnet_route_table_association" "aks_wag_association" {
  subnet_id      = azurerm_subnet.aks_subnet.id
  route_table_id = azurerm_route_table.wag_route_table.id
}

# Integrate AKS Cluster with Container Registry
resource "azurerm_role_assignment" "aks_acr_role_assignment" {
  scope                = azurerm_container_registry.acr.id
  role_definition_name = "AcrPull"
  principal_id         = azurerm_kubernetes_cluster.aks.kubelet_identity[0].object_id
}

# Integrate AKS Cluster with App Config
resource "azurerm_role_assignment" "aks_appconfig_role_assignment" {
  scope                = azurerm_app_configuration.appconfig.id
  role_definition_name = "Contributor"
  principal_id         = azurerm_kubernetes_cluster.aks.kubelet_identity[0].object_id
}

# Integrate AKS Cluster with Key Vault
resource "azurerm_role_assignment" "aks_keyvault_role_assignment" {
  scope                = azurerm_key_vault.keyvault.id
  role_definition_name = "Contributor"
  principal_id         = azurerm_kubernetes_cluster.aks.kubelet_identity[0].object_id
}

# Create Defender Plan
resource "azurerm_security_center_subscription_pricing" "defender_plan" {
  resource_group_name = azurerm_resource_group.main.name
  tier                = "Standard"
}

# Assign Defender Plan to AKS Cluster
resource "azurerm_security_center_contact" "aks_cluster_defender_contact" {
  name                = "aks-cluster-defender-contact"
  resource_group_name = azurerm_resource_group.aks_rg.name
  email               = "defender@aks-cluster.com"
  phone               = "+1234567890"
  alert_notifications = false
}

resource "azurerm_security_center_atp" "aks_cluster_defender_atp" {
  name                = "aks-cluster-defender-atp"
  resource_group_name = azurerm_resource_group.aks_rg.name
  is_enabled          = true
}

resource "azurerm_security_center_auto_provisioning" "aks_cluster_defender_auto_provisioning" {
  name                = "aks-cluster-defender-auto-provisioning"
  resource_group_name = azurerm_resource_group.aks_rg.name
  auto_provision      = true
}

resource "azurerm_security_center_assessment" "aks_cluster_defender_assessment" {
  name                = "aks-cluster-defender-assessment"
  resource_group_name = azurerm_resource_group.aks_rg.name
  assessment_type     = "Baseline"
  severity            = "High"
  categories          = ["Identity & Access", "Network Security"]
}

# Assign Kubernetes Cluster Security Baseline initiative
resource "azurerm_policy_assignment" "kubernetes_baseline" {
  name                 = "kubernetes-baseline"
  scope                = azurerm_kubernetes_cluster.aks.id
  policy_definition_id = "/providers/Microsoft.Authorization/policyDefinitions/<kubernetes_baseline_policy_definition_id>"
  enforcement_mode     = "Default"
}

# Assign Vulnerability Assessment initiative
resource "azurerm_policy_assignment" "vulnerability_assessment" {
  name                 = "vulnerability-assessment"
  scope                = azurerm_kubernetes_cluster.aks.id
  policy_definition_id = "/providers/Microsoft.Authorization/policyDefinitions/<vulnerability_assessment_policy_definition_id>"
  enforcement_mode     = "Default"
}

# Assign Logging and Monitoring initiative
resource "azurerm_policy_assignment" "logging_monitoring" {
  name                 = "logging-monitoring"
  scope                = azurerm_kubernetes_cluster.aks.id
  policy_definition_id = "/providers/Microsoft.Authorization/policyDefinitions/<logging_monitoring_policy_definition_id>"
  enforcement_mode     = "Default"
}

# Assign Compliance Standards initiative
resource "azurerm_policy_assignment" "compliance_standards" {
  name                 = "compliance-standards"
  scope                = azurerm_kubernetes_cluster.aks.id
  policy_definition_id = "/providers/Microsoft.Authorization/policyDefinitions/<compliance_standards_policy_definition_id>"
  enforcement_mode     = "Default"
}

# Assign RBAC roles for AKS cluster
resource "azurerm_role_assignment" "aks_cluster_admin" {
  scope              = azurerm_kubernetes_cluster.aks.id
  role_definition_id = data.azurerm_role_definition.aks_cluster_admin.id
  principal_id       = azurerm_user_assigned_identity.aks_cluster.identity_ids[0]
}

resource "azurerm_role_assignment" "aks_cluster_reader" {
  scope              = azurerm_kubernetes_cluster.aks.id
  role_definition_id = data.azurerm_role_definition.aks_cluster_reader.id
  principal_id       = azurerm_user_assigned_identity.aks_cluster.identity_ids[0]
}

# Assign RBAC roles for Firewall
resource "azurerm_role_assignment" "firewall_contributor" {
  scope              = azurerm_firewall.example.id
  role_definition_id = data.azurerm_role_definition.network_contributor.id
  principal_id       = azurerm_user_assigned_identity.firewall.identity_ids[0]
}

# Assign RBAC roles for Web Application Gateway
resource "azurerm_role_assignment" "webappgw_contributor" {
  scope              = azurerm_web_application_firewall.example.id
  role_definition_id = data.azurerm_role_definition.network_contributor.id
  principal_id       = azurerm_user_assigned_identity.webappgw.identity_ids[0]
}

# Assign RBAC roles for Container Registry
resource "azurerm_role_assignment" "container_registry_contributor" {
  scope              = azurerm_container_registry.example.id
  role_definition_id = data.azurerm_role_definition.container_registry_contributor.id
  principal_id       = azurerm_user_assigned_identity.container_registry.identity_ids[0]
}

# Assign RBAC roles for App Config
resource "azurerm_role_assignment" "app_config_contributor" {
  scope              = azurerm_app_configuration.example.id
  role_definition_id = data.azurerm_role_definition.app_configuration_contributor.id
  principal_id       = azurerm_user_assigned_identity.app_config.identity_ids[0]
}

# Assign RBAC roles for Key Vault
resource "azurerm_role_assignment" "key_vault_contributor" {
  scope              = azurerm_key_vault.example.id
  role_definition_id = data.azurerm_role_definition.key_vault_contributor.id
  principal_id       = azurerm_user_assigned_identity.key_vault.identity_ids[0]
}

# Assign AAD RBAC and IAM roles
resource "azurerm_role_assignment" "role_assignment_firewall" {
  scope                = azurerm_firewall.firewall.id
  role_definition_name = "Contributor"
  principal_id         = azurerm_user_assigned_identity.firewall_identity.principal_id
}

resource "azurerm_role_assignment" "role_assignment_app_gateway" {
  scope                = azurerm_application_gateway.app_gateway.id
  role_definition_name = "Contributor"
  principal_id         = azurerm_user_assigned_identity.app_gateway_identity.principal_id
}

resource "azurerm_role_assignment" "role_assignment_app_insights" {
  scope                = azurerm_application_insights.app_insights.id
  role_definition_name = "Contributor"
  principal_id         = azurerm_user_assigned_identity.app_insights_identity.principal_id
}

resource "azurerm_role_assignment" "role_assignment_event_hub" {
  scope                = azurerm_eventhub_namespace.event_hub_namespace.id
  role_definition_name = "Contributor"
  principal_id         = azurerm_user_assigned_identity.event_hub_identity.principal_id
}

resource "azurerm_role_assignment" "role_assignment_log_analytics" {
  scope                = azurerm_log_analytics_workspace.log_analytics.id
  role_definition_name = "Contributor"
  principal_id         = azurerm_user_assigned_identity.log_analytics_identity.principal_id
}

resource "azurerm_role_assignment" "role_assignment_monitor_workspace" {
  scope                = azurerm_monitor_workspace.monitor_workspace.id
  role_definition_name = "Contributor"
  principal_id         = azurerm_user_assigned_identity.monitor_workspace_identity.principal_id
}

resource "azurerm_role_assignment" "role_assignment_oms_agent" {
  scope                = azurerm_oms_agent.oms_agent.id
  role_definition_name = "Contributor"
  principal_id         = azurerm_user_assigned_identity.oms_agent_identity.principal_id
}

resource "azurerm_role_assignment" "role_assignment_container_registry" {
  scope                = azurerm_container_registry.container_registry.id
  role_definition_name = "Contributor"
  principal_id         = azurerm_user_assigned_identity.container_registry_identity.principal_id
}

resource "azurerm_role_assignment" "role_assignment_app_config" {
  scope                = azurerm_app_configuration.app_config.id
  role_definition_name = "Contributor"
  principal_id         = azurerm_user_assigned_identity.app_config_identity.principal_id
}

resource "azurerm_role_assignment" "role_assignment_key_vault" {
  scope                = azurerm_key_vault.key_vault.id
  role_definition_name = "Contributor"
  principal_id         = azurerm_user_assigned_identity.key_vault_identity.principal_id
}

############## Gatekeeper policies

# Provider configuration
provider "kubernetes" {
  config_path = "~/.kube/config"
}

# Gatekeeper Constraint Template
resource "kubernetes_constraint_template" "ingress_node" {
  metadata {
    name = "ingress-node"
  }

  spec {
    crd {
      spec {
        names {
          kind = "IngressNode"
        }

        validation {
          openAPIV3Schema {
            properties = {
              "allowed" = {
                type = "boolean"
              }
            }
          }
        }
      }
    }

    targets {
      target = "admission.k8s.gatekeeper.sh"
      rego   = <<EOF
package ingressnode

import data.kubernetes

ingressNodeMatchesAllTags[ingressNode] {
  ingressNode = kubernetes[ingressNode]
  ingressNode.allowed
}

deny[msg] {
  not ingressNodeMatchesAllTags[ingressNode]
  msg = sprintf("Node '%s' is not allowed to receive traffic from a load balancer", [ingressNode])
}

EOF
    }
  }
}

# Gatekeeper Constraint
resource "kubernetes_constraint" "example" {
  metadata {
    name = "ingress-node-constraint"
  }

  spec {
    crd {
      name = kubernetes_constraint_template.ingress_node.metadata.0.name
    }

    parameters {
      allowed = true
    }
  }
}

# Gatekeeper Config
resource "kubernetes_config" "example" {
  metadata {
    name = "example-config"
  }

  spec {
    match {
      kinds = ["Namespace"]
    }

    parameters {
      ingress-node = kubernetes_constraint.example.metadata.0.name
    }
  }
}

# Gatekeeper Config Template
resource "kubernetes_config_template" "example" {
  metadata {
    name = "example-config-template"
  }

  spec {
    crd {
      name = kubernetes_config.example.metadata.0.name
    }
  }
}

# Gatekeeper Constraint Template Binding
resource "kubernetes_constraint_template_binding" "example" {
  metadata {
    name = "example-binding"
  }

  spec {
    match {
      kinds = ["Namespace"]
    }

    parameters {
      config-template = kubernetes_config_template.example.metadata.0.name
    }
  }
}


# Gatekeeper Constraint Template
resource "kubernetes_constraint_template" "network-connectivity" {
  metadata {
    name = "network-connectivity"
  }

  spec {
    crd {
      spec {
        names {
          kind = "NetworkConnectivity"
        }

        validation {
          openAPIV3Schema {
            properties = {
              "allowedNamespaces" = {
                type = "array",
                items: {
                  type: "string"
                }
              }
            }
          }
        }
      }
    }

    targets {
      target = "admission.k8s.gatekeeper.sh"
      rego   = <<EOF
package networkconnectivity

import data.kubernetes

sharedNamespace[ns] {
  ns = "all-shared"
}

allowedNamespace[ns] {
  allowedNamespace = kubernetes[allowedNamespaces[_]]
  allowedNamespaces = {ns | ns = input.allowedNamespaces}
}

deny[msg] {
  not sharedNamespace[ns]
  not allowedNamespace[ns]
  msg = sprintf("Namespace '%s' does not have shared networking with any of the other specified namespaces", [ns])
}

EOF
    }
  }
}

# Gatekeeper Constraint
resource "kubernetes_constraint" "network_connectivity" {
  metadata {
    name = "network-connectivity-constraint"
  }

  spec {
    crd {
      name = kubernetes_constraint_template.network-connectivity.metadata.0.name
    }

    parameters {
      allowedNamespaces = [
        "all-shared",
        "production-shared",
        "production-islolated-1",
        "production-islolated-2",
        "development-shared",
        "development-isolated-1",
        "development-isolated-2"
      ]
    }
  }
}

# Gatekeeper Config
resource "kubernetes_config" "network_connectivity" {
  metadata {
    name = "network-connectivity-config"
  }

  spec {
    match {
      kinds = ["Namespace"]
    }

    parameters {
      allowedNamespaces = kubernetes_constraint.network_connectivity.metadata.0.name
    }
  }
}

# Gatekeeper Constraint Template
resource "kubernetes_constraint_template" "scheduling-restrictions" {
  metadata {
    name = "scheduling-restrictions"
  }

  spec {
    crd {
      spec {
        names {
          kind = "SchedulingRestrictions"
        }

        validation {
          openAPIV3Schema {
            properties = {
              "allowedNamespace" = {
                type = "string"
              }
            }
          }
        }
      }
    }

    targets {
      target = "admission.k8s.gatekeeper.sh"
      rego   = <<EOF
package schedulingrestrictions

import data.kubernetes

productionNamespace[ns] {
  ns = "production-shared"
}

allowedNamespace[ns] {
  allowedNamespace = input.allowedNamespace
}

deny[msg] {
  not productionNamespace[ns]
  not allowedNamespace[ns]
  msg = sprintf("Development namespace '%s' is scheduled on a node running a production namespace", [ns])
}

EOF
    }
  }
}

# Provider configuration
provider "kubernetes" {
  config_path = "~/.kube/config"
}

# Gatekeeper Constraint Template
resource "kubernetes_constraint_template" "network_connectivity" {
  metadata {
    name = "network-connectivity"
  }

  spec {
    crd {
      spec {
        names {
          kind = "NetworkConnectivity"
        }

        validation {
          openAPIV3Schema {
            properties = {
              "allowedNamespaces" = {
                type = "array",
                items: {
                  type: "string"
                }
              }
            }
          }
        }
      }
    }

    targets {
      target = "admission.k8s.gatekeeper.sh"
      rego   = <<EOF
package networkconnectivity

import data.kubernetes

sharedNamespace[ns] {
  ns = "all-shared"
}

allowedNamespace[ns] {
  allowedNamespace = kubernetes[allowedNamespaces[_]]
  allowedNamespaces = {ns | ns = input.allowedNamespaces}
}

deny[msg] {
  not sharedNamespace[ns]
  not allowedNamespace[ns]
  msg = sprintf("Namespace '%s' does not have shared networking with any of the other specified namespaces", [ns])
}

EOF
    }
  }
}

# Gatekeeper Constraint
resource "kubernetes_constraint" "network_connectivity" {
  metadata {
    name = "network-connectivity-constraint"
  }

  spec {
    crd {
      name = kubernetes_constraint_template.network_connectivity.metadata.0.name
    }

    parameters {
      allowedNamespaces = [
        "all-shared",
        "production-shared",
        "production-islolated-1",
        "production-islolated-2",
        "development-shared",
        "development-isolated-1",
        "development-isolated-2"
      ]
    }
  }
}

# Gatekeeper Config
resource "kubernetes_config" "network_connectivity" {
  metadata {
    name = "network-connectivity-config"
  }

  spec {
    match {
      kinds = ["Namespace"]
    }

    parameters {
      allowedNamespaces = kubernetes_constraint.network_connectivity.metadata.0.name
    }
  }
}

# Gatekeeper Config Template
resource "kubernetes_config_template" "network_connectivity" {
  metadata {
    name = "network-connectivity-config-template"
  }

  spec {
    crd {
      name = kubernetes_config.network_connectivity.metadata.0.name
    }
  }
}

# Gatekeeper Constraint Template Binding
resource "kubernetes_constraint_template_binding" "network_connectivity" {
  metadata {
    name = "network-connectivity-binding"
  }

  spec {
    match {
      kinds = ["Namespace"]
    }

    parameters {
      config-template = kubernetes_config_template.network_connectivity.metadata.0.name
    }
  }
}

# Gatekeeper Constraint Template
resource "kubernetes_constraint_template" "scheduling_restrictions" {
  metadata {
    name = "scheduling-restrictions"
  }

  spec {
    crd {
      spec {
        names {
          kind = "SchedulingRestrictions"
        }

        validation {
          openAPIV3Schema {
            properties = {
              "allowedNamespace" = {
                type = "string"
              }
            }
          }
        }
      }
    }

    targets {
      target = "admission.k8s.gatekeeper.sh"
      rego   = <<EOF
package schedulingrestrictions

import data.kubernetes

productionNamespace[ns] {
  ns = "production-shared"
}

allowedNamespace[ns] {
  allowedNamespace = kubernetes[allowedNamespace]
}

deny[msg] {
  not productionNamespace[productionNs]
  not allowedNamespace[developmentNs]
  developmentNs = input
  msg = sprintf("Development namespace '%s' is scheduled on a node running a production namespace", [developmentNs])
}

EOF
    }
  }
}

# Gatekeeper Constraint
resource "kubernetes_constraint" "scheduling_restrictions" {
  metadata {
    name = "scheduling-restrictions-constraint"
  }

  spec {
    crd {
      name = kubernetes_constraint_template.scheduling_restrictions.metadata.0.name
    }

    parameters {
      allowedNamespace = "development-shared"
    }
  }
}

# Gatekeeper Config
resource "kubernetes_config" "scheduling_restrictions" {
  metadata {
    name = "scheduling-restrictions-config"
  }

  spec {
    match {
      kinds = ["Namespace"]
    }

    parameters {
      allowedNamespace = kubernetes_constraint.scheduling_restrictions.metadata.0.name
    }
  }
}

# Gatekeeper Config Template
resource "kubernetes_config_template" "scheduling_restrictions" {
  metadata {
    name = "scheduling-restrictions-config-template"
  }

  spec {
    crd {
      name = kubernetes_config.scheduling_restrictions.metadata.0.name
    }
  }
}

# Gatekeeper Constraint Template Binding
resource "kubernetes_constraint_template_binding" "scheduling_restrictions" {
  metadata {
    name = "scheduling-restrictions-binding"
  }

  spec {
    match {
      kinds = ["Namespace"]
    }

    parameters {
      config-template = kubernetes_config_template.scheduling_restrictions.metadata.0.name
    }
  }
}
